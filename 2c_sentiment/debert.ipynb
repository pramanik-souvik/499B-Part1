{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5e69d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      label                                               text\n",
      "0  negative  বর্তমান সময়ে মোশারফ করিমের নাটক দেখা মানেই সময়...\n",
      "1  positive                                শেষটা অনেক ভাল লাগল\n",
      "2  positive            নামাযের চাবি ওযু আর বেহেসতের চাবি নামায\n",
      "3  positive  এটাতো কোনো সমস্যাই না। আপনি একজন ক্রেজি লাভার,...\n",
      "4  positive                   একেবারে চমৎকার তরতাজা একটা নাটক।\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Parameter 'function'=<function tokenize_function at 0x000001B5A9AE6940> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|██████████| 22/22 [00:02<00:00, 10.59ba/s]\n",
      "100%|██████████| 6/6 [00:00<00:00,  8.21ba/s]\n",
      "C:\\Users\\USERAS\\AppData\\Local\\Temp\\ipykernel_70756\\2681597053.py:79: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8199' max='8199' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8199/8199 31:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.685500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.688000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.688900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.686100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.686800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.673500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.668400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.688800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.676700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.672400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.627500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.573300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.582700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.554100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.608300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.593200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.549300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.563900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.644200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.581200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.519400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.529400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.482600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.533800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.486100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.429100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.455300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.463100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.422800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.516900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.488200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.456700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.591300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.694700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.633900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.515300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.579800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.534100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.435000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.520100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.518100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.539700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.544600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.689200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.683400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.685500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.699800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.666300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.688700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.685200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.683100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.645200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.651500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.709000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.678000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.665900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.700800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.645600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.685000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.688400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.696600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.652400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.667000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.617600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.679600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.664400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.715000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.682600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.642000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.641700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.466100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.673600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>0.679200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.692100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.686300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.662000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>0.688000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.678800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>0.672300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.683500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>0.662900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>0.656500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>0.684900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.674200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.702600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>0.655300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>0.684400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.661000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>0.667500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.632600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>0.651700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.660900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>0.659800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>0.607200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.712100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.678200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>0.677200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>0.697800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>0.628200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.683300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>0.657500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.672200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>0.708600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.675600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.681000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>0.674700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>0.676900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.688300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>0.676200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.667700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>0.675800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.653200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>0.673000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.693000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.651200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>0.695200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>0.681900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>0.689000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>0.682500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.661400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>0.666200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>0.670300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>0.658400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>0.690100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.649000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.675700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2540</td>\n",
       "      <td>0.664500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>0.645200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2580</td>\n",
       "      <td>0.647400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.700400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2620</td>\n",
       "      <td>0.666600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>0.677900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2660</td>\n",
       "      <td>0.690200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>0.656600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.665300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>0.669600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2740</td>\n",
       "      <td>0.693800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>0.673400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2780</td>\n",
       "      <td>0.676300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.677100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2820</td>\n",
       "      <td>0.668200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2840</td>\n",
       "      <td>0.664500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2860</td>\n",
       "      <td>0.688100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>0.650500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.678400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2920</td>\n",
       "      <td>0.691200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2940</td>\n",
       "      <td>0.681200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2960</td>\n",
       "      <td>0.678700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2980</td>\n",
       "      <td>0.669200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.668300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3020</td>\n",
       "      <td>0.691600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>0.668200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3060</td>\n",
       "      <td>0.663100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3080</td>\n",
       "      <td>0.631200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.667400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>0.667100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3140</td>\n",
       "      <td>0.683900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3160</td>\n",
       "      <td>0.671700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3180</td>\n",
       "      <td>0.679500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.678400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3220</td>\n",
       "      <td>0.692700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>0.643600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3260</td>\n",
       "      <td>0.651800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3280</td>\n",
       "      <td>0.671700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.710100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3320</td>\n",
       "      <td>0.659500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3340</td>\n",
       "      <td>0.654600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3360</td>\n",
       "      <td>0.717600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3380</td>\n",
       "      <td>0.690800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.648700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3420</td>\n",
       "      <td>0.678800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3440</td>\n",
       "      <td>0.673000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3460</td>\n",
       "      <td>0.630100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3480</td>\n",
       "      <td>0.695300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.656000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3520</td>\n",
       "      <td>0.700600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3540</td>\n",
       "      <td>0.663100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3560</td>\n",
       "      <td>0.686500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3580</td>\n",
       "      <td>0.685000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.681400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3620</td>\n",
       "      <td>0.660400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3640</td>\n",
       "      <td>0.690200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3660</td>\n",
       "      <td>0.645300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3680</td>\n",
       "      <td>0.682000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.683500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3720</td>\n",
       "      <td>0.638600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3740</td>\n",
       "      <td>0.663500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3760</td>\n",
       "      <td>0.661300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3780</td>\n",
       "      <td>0.701700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.664100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3820</td>\n",
       "      <td>0.691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3840</td>\n",
       "      <td>0.682600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3860</td>\n",
       "      <td>0.671200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3880</td>\n",
       "      <td>0.659500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.657800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3920</td>\n",
       "      <td>0.653600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3940</td>\n",
       "      <td>0.680100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3960</td>\n",
       "      <td>0.646100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3980</td>\n",
       "      <td>0.652400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.675400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4020</td>\n",
       "      <td>0.690400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4040</td>\n",
       "      <td>0.620600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4060</td>\n",
       "      <td>0.686700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4080</td>\n",
       "      <td>0.693400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.652400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4120</td>\n",
       "      <td>0.699700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4140</td>\n",
       "      <td>0.666900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4160</td>\n",
       "      <td>0.672600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4180</td>\n",
       "      <td>0.657600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.711700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4220</td>\n",
       "      <td>0.650700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4240</td>\n",
       "      <td>0.640600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4260</td>\n",
       "      <td>0.697200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4280</td>\n",
       "      <td>0.647100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.687000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4320</td>\n",
       "      <td>0.650700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4340</td>\n",
       "      <td>0.686700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4360</td>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4380</td>\n",
       "      <td>0.708500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.686900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4420</td>\n",
       "      <td>0.647400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4440</td>\n",
       "      <td>0.664400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4460</td>\n",
       "      <td>0.677100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4480</td>\n",
       "      <td>0.671100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.665400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4520</td>\n",
       "      <td>0.682400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4540</td>\n",
       "      <td>0.681200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4560</td>\n",
       "      <td>0.655200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4580</td>\n",
       "      <td>0.679600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.671300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4620</td>\n",
       "      <td>0.706600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4640</td>\n",
       "      <td>0.665800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4660</td>\n",
       "      <td>0.691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4680</td>\n",
       "      <td>0.661500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.660600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4720</td>\n",
       "      <td>0.668100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4740</td>\n",
       "      <td>0.659000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4760</td>\n",
       "      <td>0.674400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4780</td>\n",
       "      <td>0.646800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.666300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4820</td>\n",
       "      <td>0.709500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4840</td>\n",
       "      <td>0.681800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4860</td>\n",
       "      <td>0.663000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4880</td>\n",
       "      <td>0.658900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.662000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4920</td>\n",
       "      <td>0.683000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4940</td>\n",
       "      <td>0.682700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4960</td>\n",
       "      <td>0.671900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4980</td>\n",
       "      <td>0.689700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.684200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5020</td>\n",
       "      <td>0.696000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5040</td>\n",
       "      <td>0.685900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5060</td>\n",
       "      <td>0.665100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5080</td>\n",
       "      <td>0.704900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.658500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5120</td>\n",
       "      <td>0.668300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5140</td>\n",
       "      <td>0.676900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5160</td>\n",
       "      <td>0.646400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5180</td>\n",
       "      <td>0.680300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.658400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5220</td>\n",
       "      <td>0.671100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5240</td>\n",
       "      <td>0.682900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5260</td>\n",
       "      <td>0.654400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5280</td>\n",
       "      <td>0.672600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.668600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5320</td>\n",
       "      <td>0.668400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5340</td>\n",
       "      <td>0.668900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5360</td>\n",
       "      <td>0.688300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5380</td>\n",
       "      <td>0.657500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.648400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5420</td>\n",
       "      <td>0.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5440</td>\n",
       "      <td>0.694200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5460</td>\n",
       "      <td>0.681100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5480</td>\n",
       "      <td>0.663800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.698300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5520</td>\n",
       "      <td>0.658900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5540</td>\n",
       "      <td>0.694300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5560</td>\n",
       "      <td>0.657000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5580</td>\n",
       "      <td>0.724000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.535100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5620</td>\n",
       "      <td>0.528800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5640</td>\n",
       "      <td>0.567100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5660</td>\n",
       "      <td>0.509900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5680</td>\n",
       "      <td>0.449700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.487700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5720</td>\n",
       "      <td>0.429700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5740</td>\n",
       "      <td>0.540600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5760</td>\n",
       "      <td>0.480200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5780</td>\n",
       "      <td>0.415500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.418900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5820</td>\n",
       "      <td>0.419200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5840</td>\n",
       "      <td>0.412900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5860</td>\n",
       "      <td>0.351500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5880</td>\n",
       "      <td>0.350100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.429300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5920</td>\n",
       "      <td>0.513700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5940</td>\n",
       "      <td>0.482600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5960</td>\n",
       "      <td>0.459200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5980</td>\n",
       "      <td>0.437000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.400200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6020</td>\n",
       "      <td>0.339600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6040</td>\n",
       "      <td>0.497700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6060</td>\n",
       "      <td>0.357100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6080</td>\n",
       "      <td>0.387200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.370300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6120</td>\n",
       "      <td>0.384200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6140</td>\n",
       "      <td>0.414300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6160</td>\n",
       "      <td>0.387000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6180</td>\n",
       "      <td>0.463200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.413100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6220</td>\n",
       "      <td>0.320100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6240</td>\n",
       "      <td>0.384600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6260</td>\n",
       "      <td>0.456000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6280</td>\n",
       "      <td>0.397800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.392700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6320</td>\n",
       "      <td>0.392200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6340</td>\n",
       "      <td>0.402900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6360</td>\n",
       "      <td>0.368900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6380</td>\n",
       "      <td>0.314400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.312700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6420</td>\n",
       "      <td>0.463300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6440</td>\n",
       "      <td>0.265400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6460</td>\n",
       "      <td>0.382200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6480</td>\n",
       "      <td>0.262200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.380500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6520</td>\n",
       "      <td>0.427200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6540</td>\n",
       "      <td>0.346200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6560</td>\n",
       "      <td>0.387700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6580</td>\n",
       "      <td>0.450700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.361600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6620</td>\n",
       "      <td>0.330300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6640</td>\n",
       "      <td>0.336100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6660</td>\n",
       "      <td>0.390500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6680</td>\n",
       "      <td>0.356700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.311700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6720</td>\n",
       "      <td>0.432300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6740</td>\n",
       "      <td>0.324900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6760</td>\n",
       "      <td>0.291300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6780</td>\n",
       "      <td>0.321900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.392000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6820</td>\n",
       "      <td>0.328200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6840</td>\n",
       "      <td>0.384600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6860</td>\n",
       "      <td>0.300300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6880</td>\n",
       "      <td>0.358600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.365700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6920</td>\n",
       "      <td>0.325500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6940</td>\n",
       "      <td>0.280600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6960</td>\n",
       "      <td>0.335200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6980</td>\n",
       "      <td>0.360500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.462700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7020</td>\n",
       "      <td>0.351900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7040</td>\n",
       "      <td>0.334100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7060</td>\n",
       "      <td>0.325500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7080</td>\n",
       "      <td>0.325300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.322100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7120</td>\n",
       "      <td>0.301500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7140</td>\n",
       "      <td>0.309800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7160</td>\n",
       "      <td>0.286200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7180</td>\n",
       "      <td>0.390100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.294800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7220</td>\n",
       "      <td>0.211400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7240</td>\n",
       "      <td>0.355100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7260</td>\n",
       "      <td>0.393400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7280</td>\n",
       "      <td>0.374800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.326600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7320</td>\n",
       "      <td>0.308700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7340</td>\n",
       "      <td>0.296500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7360</td>\n",
       "      <td>0.411700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7380</td>\n",
       "      <td>0.297600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.237400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7420</td>\n",
       "      <td>0.237500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7440</td>\n",
       "      <td>0.327500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7460</td>\n",
       "      <td>0.379900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7480</td>\n",
       "      <td>0.351300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.267000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7520</td>\n",
       "      <td>0.306800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7540</td>\n",
       "      <td>0.394400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7560</td>\n",
       "      <td>0.335400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7580</td>\n",
       "      <td>0.281900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.281100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7620</td>\n",
       "      <td>0.299100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7640</td>\n",
       "      <td>0.279700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7660</td>\n",
       "      <td>0.389900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7680</td>\n",
       "      <td>0.422200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.377700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7720</td>\n",
       "      <td>0.322700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7740</td>\n",
       "      <td>0.308200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7760</td>\n",
       "      <td>0.301200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7780</td>\n",
       "      <td>0.229600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.372100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7820</td>\n",
       "      <td>0.282000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7840</td>\n",
       "      <td>0.367400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7860</td>\n",
       "      <td>0.317100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7880</td>\n",
       "      <td>0.514100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.386100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7920</td>\n",
       "      <td>0.321200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7940</td>\n",
       "      <td>0.267100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7960</td>\n",
       "      <td>0.333700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7980</td>\n",
       "      <td>0.302200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.232800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8020</td>\n",
       "      <td>0.301700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8040</td>\n",
       "      <td>0.497100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8060</td>\n",
       "      <td>0.368400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8080</td>\n",
       "      <td>0.359000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.278200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8120</td>\n",
       "      <td>0.181600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8140</td>\n",
       "      <td>0.343300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8160</td>\n",
       "      <td>0.295900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8180</td>\n",
       "      <td>0.356600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluation results: {'eval_loss': 0.2863825559616089, 'eval_accuracy': 0.8871410279860984, 'eval_f1': 0.8875525570141706, 'eval_runtime': 26.7745, 'eval_samples_per_second': 204.187, 'eval_steps_per_second': 51.056, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\transformers\\utils\\generic.py:271: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('2c_sentiment.csv')\n",
    "df = df.dropna()\n",
    "data = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(data.head())\n",
    "X = data['text']\n",
    "y = data['label']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y.astype(str))\n",
    "\n",
    "# Create Hugging Face dataset\n",
    "dataset = Dataset.from_pandas(pd.DataFrame({'text': X, 'label': y_encoded}))\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Load BanglaBERT tokenizer and model\n",
    "model_name = 'microsoft/deberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=len(label_encoder.classes_)\n",
    ")\n",
    "\n",
    "# Tokenize\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "dataset = dataset.remove_columns(['text']).rename_column('label', 'labels')\n",
    "dataset.set_format('torch')\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    # evaluation_strategy='epoch',\n",
    "    # save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=20,\n",
    "    # load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    greater_is_better=True,\n",
    "    warmup_ratio=0.1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    labels = eval_pred.label_ids\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions, average='weighted')\n",
    "    }\n",
    "\n",
    "# Trainer with early stopping\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    "    # callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"\\n Evaluation results: {eval_results}\")\n",
    "\n",
    "# Predictions\n",
    "predictions_output = trainer.predict(dataset['test'])\n",
    "predictions = np.argmax(predictions_output.predictions, axis=1)\n",
    "labels = predictions_output.label_ids\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62cff573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.88      0.86      2163\n",
      "    positive       0.92      0.89      0.91      3304\n",
      "\n",
      "    accuracy                           0.89      5467\n",
      "   macro avg       0.88      0.89      0.88      5467\n",
      "weighted avg       0.89      0.89      0.89      5467\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Classification report\n",
    "used_labels = sorted(unique_labels(labels, predictions))\n",
    "used_class_names = [str(label_encoder.classes_[i]) for i in used_labels]\n",
    "\n",
    "report = classification_report(\n",
    "    labels, predictions,\n",
    "    labels=used_labels,\n",
    "    target_names=used_class_names,\n",
    "    zero_division=0\n",
    ")\n",
    "print(\" Classification Report:\\n\", report)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(labels, predictions, labels=used_labels)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=used_class_names, yticklabels=used_class_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fff96132",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '5c_emotion.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m5c_emotion.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[0;32m     20\u001b[0m data \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\USERAS\\anaconda3\\envs\\resPy\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '5c_emotion.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('5c_emotion.csv')\n",
    "df = df.dropna()\n",
    "data = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(data.head())\n",
    "X = data['text']\n",
    "y = data['label']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y.astype(str))\n",
    "\n",
    "# Create Hugging Face dataset\n",
    "dataset = Dataset.from_pandas(pd.DataFrame({'text': X, 'label': y_encoded}))\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Load BanglaBERT tokenizer and model\n",
    "device = torch.device(\"cuda\")\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=len(label_encoder.classes_)\n",
    "\n",
    ").to(device)\n",
    "\n",
    "# Tokenize\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "dataset = dataset.remove_columns(['text']).rename_column('label', 'labels')\n",
    "dataset.set_format('torch')\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    # evaluation_strategy='epoch',\n",
    "    # save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=20,\n",
    "    # load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    greater_is_better=True,\n",
    "    warmup_ratio=0.1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    labels = eval_pred.label_ids\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions, average='weighted')\n",
    "    }\n",
    "\n",
    "# Trainer with early stopping\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    "    # callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"\\n Evaluation results: {eval_results}\")\n",
    "\n",
    "# Predictions\n",
    "predictions_output = trainer.predict(dataset['test'])\n",
    "predictions = np.argmax(predictions_output.predictions, axis=1)\n",
    "labels = predictions_output.label_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7e2ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Classification report\n",
    "used_labels = sorted(unique_labels(labels, predictions))\n",
    "used_class_names = [str(label_encoder.classes_[i]) for i in used_labels]\n",
    "\n",
    "report = classification_report(\n",
    "    labels, predictions,\n",
    "    labels=used_labels,\n",
    "    target_names=used_class_names,\n",
    "    zero_division=0\n",
    ")\n",
    "print(\" Classification Report:\\n\", report)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(labels, predictions, labels=used_labels)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=used_class_names, yticklabels=used_class_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resPy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
